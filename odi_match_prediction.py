# -*- coding: utf-8 -*-
"""ODI_Match_Prediction (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nllNDVwEeGeFLYosq-bw4gf-shH6zFy0
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/odi.csv')

df.head()

"""## EDA – Exploratory Data Analysis"""

df.shape

df.info()

df.isnull().sum()

df.describe()

df.corr()

df[['date','venue', 'bat_team', 'bowl_team', 'batsman', 'bowler',
       'runs', 'wickets', 'overs', 'runs_last_5', 'wickets_last_5', 'striker',
       'non-striker']][:50].plot(kind = 'area', figsize = (10,10), stacked = False)

"""# Data Cleaning"""

# Removing unwanted cols - reduce memory size
cols_to_remove = ['mid' ,'date','batsman', 'bowler', 'striker', 'non-striker']
df.drop(labels=cols_to_remove , axis=1 , inplace = True)

df.head()

df['remaining_overs'] = 49.6 - df['overs']

df['weight_overs'] = (df['remaining_overs'] / 49.6)

df['remaining_wickets'] = 10 - df['wickets']

df['weight_wicket'] = (df['remaining_wickets'] / 10)

df['merge_weight'] = (df['remaining_overs']*df['weight_overs']) + (df['remaining_wickets']*df['weight_wicket'])

# import pandas as pd
# from google.colab import files

# # Assuming you have a DataFrame 'df' that you want to save
# df.to_csv('file_name.csv', index=False)

# # Download the CSV file to your local machine
# files.download('file_name.csv')

df.head(50)

#Extract rows with venues where the game is played min 5 times
df["venue"].value_counts()[df["venue"].value_counts() > 1500].sum()

elig_venue = df["venue"].value_counts()[df["venue"].value_counts() > 1500].index.tolist()

df = df[df["venue"].isin(elig_venue)]
df.shape

data = df[["bat_team"]].value_counts()
data

data2 = df[["bowl_team"]].value_counts()
data2

consistent_team = ['England', 'Pakistan', 'Sri Lanka', 'Australia', 'South Africa',
       'New Zealand', 'Bangladesh', 'West Indies', 'India',
       'Afghanistan','Zimbabwe','Ireland']

df['bat_team'].unique()

df = df[(df['bat_team'].isin(consistent_team)) & (df['bowl_team'].isin(consistent_team))]

df = df[df['overs']>=5.0]

df

df["over"] = df["overs"].apply(lambda x: str(x).split(".")[0])
df["ball_no"] = df["overs"].apply(lambda x: str(x).split(".")[1])

df.head(10)

df['balls_bowled'] = (df['over'].astype('int')*6) + df['ball_no'].astype('int')

df["runrate"] = (df["runs"]*6 ) / df["balls_bowled"]

df.head(50)

df.corr()

final_df = df[['venue','bat_team','bowl_team','overs','runs','wickets','runrate','runs_last_5','wickets_last_5','merge_weight', 'total' ]]
final_df

final_df.info()

final_df.corr()

import numpy as np
pd.np.random.seed(42)
final_df = final_df.sample(frac=1).reset_index(drop=True)

final_df

import matplotlib.pyplot as plt

plt.ylabel('total')
plt.title(f'Scatter Plot of others vs. total')
final_df.corr()['total'][:-1].sort_values().plot(kind='bar')

final_df["venue"].value_counts()[final_df["venue"].value_counts() > 7500].sum()

elig_venue1 = final_df["venue"].value_counts()[final_df["venue"].value_counts() > 9000].index.tolist()
cat_df = final_df[final_df["venue"].isin(elig_venue1)]
cat_df.shape

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler


trf = ColumnTransformer([
    ('trf',OneHotEncoder(sparse=False,drop='first'),['bat_team','bowl_team','venue'])
]
,remainder='passthrough')

Y1 = final_df.total
X1 = final_df.drop('total', axis=1)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X1,Y1, test_size=0.2, random_state=100)

"""# XGBRegressor"""

from sklearn import metrics
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score

pipe = Pipeline(steps=[
    ('step1',trf),
    ('step2',StandardScaler()),
    # ('step3',XGBRegressor(n_estimators=200, objective= "reg:squarederror", learning_rate=0.05,max_depth=10, alpha=1, colsample_bytree=0.5, subsample=0.5))-> 88 percent
    # ('step3',XGBRegressor(n_estimators=200, objective= "reg:squarederror", learning_rate=0.1,max_depth=10, alpha=0.1, colsample_bytree=0.5, subsample=0.5)) -> 93 percent
    # ('step3',XGBRegressor(n_estimators=500, objective= "reg:squarederror", learning_rate=0.1,max_depth=9, alpha=2, colsample_bytree=0.3, subsample=0.8)) -> 94 percent
    # ('step3',XGBRegressor(n_estimators=700, objective= "reg:squarederror", learning_rate=0.1,max_depth=10, alpha=2, colsample_bytree=0.3, subsample=0.5)) -> mse diff:50>
    ('step3',XGBRegressor(n_estimators=700, objective= "reg:squarederror", learning_rate=0.1,max_depth=10, alpha=2, colsample_bytree=0.5, subsample=0.8))
])

# fit the model with the training data
pipe.fit(X_train,y_train)

# Make predictions on the training data
y_train_pred = pipe.predict(X_train)
mae_train = metrics.mean_absolute_error(y_train, y_train_pred)
mse_train = metrics.mean_squared_error(y_train, y_train_pred)
rmse_train = metrics.mean_squared_error(y_train, y_train_pred, squared=False)
r2_train = metrics.r2_score(y_train, y_train_pred)

# Make predictions on the testing data
y_test_pred = pipe.predict(X_test)

mae_test = metrics.mean_absolute_error(y_test, y_test_pred)
mse_test = metrics.mean_squared_error(y_test, y_test_pred)
rmse_test = metrics.mean_squared_error(y_test, y_test_pred, squared=False)
r2_test = metrics.r2_score(y_test, y_test_pred)

# Print the metrics
print("Training Metrics:")
print(f"MAE: {mae_train}")
print(f"MSE: {mse_train}")
print(f"RMSE: {rmse_train}")
print(f"R-squared (R²): {r2_train}")

print("\nTesting Metrics:")
print(f"MAE: {mae_test}")
print(f"MSE: {mse_test}")
print(f"RMSE: {rmse_test}")
print(f"R-squared (R²): {r2_test}")

import seaborn as sns
sns.distplot(y_test_pred)

import pickle
with open('pipe_xgb.pkl', 'wb') as file:
    pickle.dump(pipe, file)

"""# Linear Regression"""

from sklearn.linear_model import LinearRegression
from sklearn import metrics
import numpy as np
from sklearn import metrics

reg = LinearRegression()

pipe_reg = Pipeline(steps=[
    ('step1',trf),
    ('step3',reg)
])

pipe_reg.fit(X_train , y_train)

# Make predictions on the training data
y_train_pred1 = pipe_reg.predict(X_train)

mae_train = metrics.mean_absolute_error(y_train, y_train_pred1)
mse_train = metrics.mean_squared_error(y_train, y_train_pred1)
rmse_train = metrics.mean_squared_error(y_train, y_train_pred1, squared=False)
r2_train = metrics.r2_score(y_train, y_train_pred1)

# Make predictions on the test data
y_test_pred1 = pipe_reg.predict(X_test)

mae_test = metrics.mean_absolute_error(y_test, y_test_pred1)
mse_test = metrics.mean_squared_error(y_test, y_test_pred1)
rmse_test = metrics.mean_squared_error(y_test, y_test_pred1, squared=False)
r2_test = metrics.r2_score(y_test, y_test_pred1)

# Print the metrics
print("Training Metrics:")
print(f"MAE: {mae_train}")
print(f"MSE: {mse_train}")
print(f"RMSE: {rmse_train}")
print(f"R-squared (R²): {r2_train}")
print("\nTesting Metrics:")
print(f"MAE: {mae_test}")
print(f"MSE: {mse_test}")
print(f"RMSE: {rmse_test}")
print(f"R-squared (R²): {r2_test}")

import seaborn as sns
sns.distplot(y_test_pred1)

"""# ElasticNet Regression"""

from sklearn.linear_model import ElasticNet
from sklearn import metrics
import numpy as np
from sklearn import metrics

elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.9)

pipe_reg2 = Pipeline(steps=[
    ('step1',trf),
    ('step3',elastic_net)
])

pipe_reg2.fit(X_train , y_train)

# Make predictions on the training data
y_train_pred2 = pipe_reg2.predict(X_train)

mae_train = metrics.mean_absolute_error(y_train, y_train_pred2)
mse_train = metrics.mean_squared_error(y_train, y_train_pred2)
rmse_train = metrics.mean_squared_error(y_train, y_train_pred2, squared=False)
r2_train = metrics.r2_score(y_train, y_train_pred2)

# Make predictions on the test data
y_test_pred2 = pipe_reg2.predict(X_test)

mae_test = metrics.mean_absolute_error(y_test, y_test_pred2)
mse_test = metrics.mean_squared_error(y_test, y_test_pred2)
rmse_test = metrics.mean_squared_error(y_test, y_test_pred2, squared=False)
r2_test = metrics.r2_score(y_test, y_test_pred2)

# Print the metrics
print("Training Metrics:")
print(f"MAE: {mae_train}")
print(f"MSE: {mse_train}")
print(f"RMSE: {rmse_train}")
print(f"R-squared (R²): {r2_train}")
print("\nTesting Metrics:")
print(f"MAE: {mae_test}")
print(f"MSE: {mse_test}")
print(f"RMSE: {rmse_test}")
print(f"R-squared (R²): {r2_test}")

import seaborn as sns
sns.distplot(y_test_pred2)

"""# Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
import numpy as np
from sklearn import metrics

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

pipe_reg3 = Pipeline(steps=[
    ('step1',trf),
    ('step3',rf_regressor)
])

pipe_reg3.fit(X_train , y_train)

# Make predictions on the training data
y_train_pred3 = pipe_reg3.predict(X_train)

mae_train = metrics.mean_absolute_error(y_train, y_train_pred3)
mse_train = metrics.mean_squared_error(y_train, y_train_pred3)
rmse_train = metrics.mean_squared_error(y_train, y_train_pred3, squared=False)
r2_train = metrics.r2_score(y_train, y_train_pred3)

# Make predictions on the test data
y_test_pred3 = pipe_reg3.predict(X_test)

mae_test = metrics.mean_absolute_error(y_test, y_test_pred3)
mse_test = metrics.mean_squared_error(y_test, y_test_pred3)
rmse_test = metrics.mean_squared_error(y_test, y_test_pred3, squared=False)
r2_test = metrics.r2_score(y_test, y_test_pred3)

# Print the metrics
print("Training Metrics:")
print(f"MAE: {mae_train}")
print(f"MSE: {mse_train}")
print(f"RMSE: {rmse_train}")
print(f"R-squared (R²): {r2_train}")
print("\nTesting Metrics:")
print(f"MAE: {mae_test}")
print(f"MSE: {mse_test}")
print(f"RMSE: {rmse_test}")
print(f"R-squared (R²): {r2_test}")

import pickle
with open('pipe_randomforest.pkl', 'wb') as file:
    pickle.dump(pipe_reg3, file)

import seaborn as sns
sns.distplot(y_test_pred3)

"""# Polynomial regression"""

cat_df

cat_df = pd.get_dummies(data = cat_df, columns = ['bat_team' , 'bowl_team', 'venue'])
cat_df.head(5)
Y = cat_df.total
X = cat_df.drop('total', axis=1)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=100)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn import metrics

# Create a Polynomial Regression model with a degree of 3
polyreg3 = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())

# Train the Polynomial Regression model on the training data
polyreg3.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred4 = polyreg3.predict(X_train)

mae_train = metrics.mean_absolute_error(y_train, y_train_pred4)
mse_train = metrics.mean_squared_error(y_train, y_train_pred4)
rmse_train = metrics.mean_squared_error(y_train, y_train_pred4, squared=False)
r2_train = metrics.r2_score(y_train, y_train_pred4)

# Make predictions on the test data
y_test_pred4 = polyreg3.predict(X_test)

mae_test = metrics.mean_absolute_error(y_test, y_test_pred4)
mse_test = metrics.mean_squared_error(y_test, y_test_pred4)
rmse_test = metrics.mean_squared_error(y_test, y_test_pred4, squared=False)
r2_test = metrics.r2_score(y_test, y_test_pred4)

import seaborn as sns
sns.distplot(y_test_pred4)